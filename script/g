import torch
import torch.nn as nn
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from tabpfn import TabPFNClassifier # Utilis√© pour l'extraction du moteur

# ==========================================
# 1. STRUCTURE DU MOD√àLE (Copi√©e de Step 8 pour √©viter l'ImportError)
# ==========================================
class TabPFNSABRRegressor(nn.Module):
    def __init__(self, n_out=7, hidden_dims=[512, 256, 128], activation='swish'):
        super().__init__()
        # Initialisation propre de TabPFN v2
        base_model = TabPFNClassifier(device='cpu')
        # On force un petit fit pour charger l'attribut interne model_
        base_model.fit(np.random.randn(5, 8), np.random.randint(0, 2, 5))
        
        # Acc√®s s√©curis√© √† l'encodeur Transformer
        self.encoder = base_model.model_[2] if hasattr(base_model, 'model_') else base_model.model[2]
        d_model = getattr(self.encoder, 'emsize', 512)
        
        # T√™te MLP personnalis√©e
        layers = []
        prev_dim = d_model
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.SiLU() if activation == 'swish' else nn.GELU(),
                nn.BatchNorm1d(h_dim)
            ])
            prev_dim = h_dim
        layers.append(nn.Linear(prev_dim, n_out))
        self.custom_head = nn.Sequential(*layers)

    def forward(self, x):
        # Format attendu par TabPFN: [S√©quence, Batch, Features]
        x_in = x.unsqueeze(0)
        dummy_y = torch.zeros(1, x.size(0), 1).to(x.device)
        # Extraction des caract√©ristiques par le Transformer
        features = self.encoder(x_in, dummy_y)
        # Passage dans la t√™te MLP sp√©cialis√©e
        return self.custom_head(features.squeeze(0))

# ==========================================
# 2. G√âN√âRATEUR CAUSAL (Simulation Infinie)
# ==========================================
class SABRCausalGenerator:
    def __init__(self, beta=0.5):
        self.beta = beta

    def generate(self, batch_size=32):
        alpha = np.random.uniform(0.1, 0.4, (batch_size, 1))
        rho = np.random.uniform(-0.8, -0.2, (batch_size, 1))
        volvol = np.random.uniform(0.2, 0.6, (batch_size, 1))
        F = np.random.uniform(95, 105, (batch_size, 1))
        K = np.random.uniform(85, 115, (batch_size, 1))
        T = np.random.uniform(0.1, 1.5, (batch_size, 1))
        log_moneyness = np.log(K / F)

        # Relation Causale (Physique du SABR)
        vol = alpha * (1 + (volvol**2 * (2-3*rho**2)/24) * T)
        
        # Greeks simul√©s (D√©riv√©es pour Sobolev)
        y = np.hstack([vol, np.ones((batch_size, 6)) * 0.5]) # Placeholder pour d√©mo
        X = np.hstack([np.full((batch_size, 1), self.beta), rho, volvol, alpha, alpha, F, K, log_moneyness])
        return torch.FloatTensor(X), torch.FloatTensor(y)

# ==========================================
# 3. EX√âCUTION DU FINE-TUNING ET DU DUEL
# ==========================================
if __name__ == "__main__":
    # Chargement config Step 7
    config_path = Path("ray_results/best_config.json")
    if config_path.exists():
        with open(config_path, "r") as f:
            config = json.load(f)
    else:
        config = {'hidden_dims': [512, 256, 128], 'lr': 0.001, 'activation': 'swish', 'value_weight': 1.0, 'derivative_weight': 0.1, 'batch_size': 32}

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TabPFNSABRRegressor(hidden_dims=config['hidden_dims'], activation=config['activation']).to(device)
    
    # Tentative de chargement du mod√®le Step 8
    try:
        model.load_state_dict(torch.load("tabpfn_sabr_final.pth", map_location=device))
        print("‚úÖ Base Step 8 charg√©e. D√©but de l'am√©lioration Causale...")
    except:
        print("‚ÑπÔ∏è D√©part √† z√©ro (tabpfn_sabr_final.pth non trouv√©).")

    # Autoriser le Fine-tuning complet
    for param in model.parameters():
        param.requires_grad = True

    generator = SABRCausalGenerator()
    from step6_loss_with_derivatives import create_loss_function
    criterion = create_loss_function(loss_type='derivative', value_weight=config['value_weight'], derivative_weight=config['derivative_weight'])
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'] * 0.1)

    # Entra√Ænement
    model.train()
    for step in range(201):
        X_batch, y_batch = generator.generate(batch_size=config['batch_size'])
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        out = model(X_batch)
        loss, _ = criterion(out[:, 0:1], y_batch[:, 0:1], {f'd{i}': out[:, i:i+1] for i in range(1, 7)}, {f'd{i}': y_batch[:, i:i+1] for i in range(1, 7)})
        loss.backward()
        optimizer.step()
        if step % 50 == 0: print(f"Step {step} | Loss Causal: {loss.item():.6f}")

    # --- LE GRAPHIQUE DE COMPARAISON (PERFORMANCE) ---
    model.eval()
    X_test, y_true = generator.generate(batch_size=100)
    idx = X_test[:, -1].argsort()
    X_test, y_true = X_test[idx], y_true[idx]
    
    with torch.no_grad():
        y_pred = model(X_test.to(device)).cpu().numpy()

    plt.figure(figsize=(10, 6))
    plt.plot(X_test[:, -1], y_true[:, 0], 'k--', label='V√©rit√© Terrain (Graphe Causal)')
    plt.plot(X_test[:, -1], y_pred[:, 0], 'r-', label='Ton TabPFN Modifi√©', linewidth=2)
    plt.title("Step 9 : Performance du Mod√®le Causal Fine-tun√©")
    plt.xlabel("Log-Moneyness")
    plt.ylabel("Volatilit√©")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig("performance_duel_step9.png")
    plt.show()

    torch.save(model.state_dict(), "tabpfn_sabr_causal_final.pth")
    print("üöÄ Mod√®le Causal Final sauvegard√© sous 'tabpfn_sabr_causal_final.pth'")