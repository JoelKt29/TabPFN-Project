import torch
import torch.nn as nn
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Import de ton mod√®le et de tes fonctions de perte
from step8_transfer_learning import TabPFNSABRRegressor
from step6_loss_with_derivatives import create_loss_function

# ==========================================
# 1. LE G√âN√âRATEUR CAUSAL (Moteur physique)
# ==========================================
class SABRCausalGenerator:
    """G√©n√©rateur de donn√©es synth√©tiques respectant les √©quations du SABR."""
    def __init__(self, beta=0.5):
        self.beta = beta

    def generate(self, batch_size=32):
        # Param√®tres d'entr√©e (Causes)
        alpha = np.random.uniform(0.1, 0.4, (batch_size, 1))
        rho = np.random.uniform(-0.8, -0.2, (batch_size, 1))
        volvol = np.random.uniform(0.2, 0.6, (batch_size, 1))
        F = np.random.uniform(95, 105, (batch_size, 1))
        K = np.random.uniform(85, 115, (batch_size, 1))
        T = np.random.uniform(0.1, 1.5, (batch_size, 1))
        v_atm_n = alpha * (F ** (self.beta - 1))
        log_moneyness = np.log(K / F)

        # Calcul de la Volatilit√© (Effet) - Formule de Hagan simplifi√©e
        vol = alpha * (1 + (volvol**2 * (2-3*rho**2)/24) * T)
        
        # D√©riv√©es (Greeks)
        dV_dalpha = np.ones_like(vol) * 1.1
        dV_drho = np.ones_like(vol) * 0.4
        dV_dvolvol = np.ones_like(vol) * 0.7
        dV_dF = (vol / F) * -0.05
        dV_dK = (vol / K) * 0.05
        dV_dT = np.ones_like(vol) * 0.02

        X = np.hstack([np.full((batch_size, 1), self.beta), rho, volvol, v_atm_n, alpha, F, K, log_moneyness])
        y = np.hstack([vol, dV_dalpha, dV_drho, dV_dvolvol, dV_dF, dV_dK, dV_dT])
        
        return torch.FloatTensor(X), torch.FloatTensor(y)

# ==========================================
# 2. CHARGEMENT ET CONFIGURATION
# ==========================================
with open("ray_results/best_config.json", "r") as f:
    config = json.load(f)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Instanciation du mod√®le import√© de Step 8
model = TabPFNSABRRegressor(hidden_dims=config['hidden_dims']).to(device)

# Chargement des poids entra√Æn√©s en Step 8 (statique)
try:
    model.load_state_dict(torch.load("tabpfn_sabr_final.pth", map_location=device))
    print("‚úÖ Poids de la Step 8 charg√©s avec succ√®s.")
except FileNotFoundError:
    print("‚ö†Ô∏è Attention : tabpfn_sabr_final.pth non trouv√©, d√©part √† z√©ro.")

# D√âBLOCAGE DES POIDS : On autorise le Transformer √† apprendre
for param in model.parameters():
    param.requires_grad = True

# ==========================================
# 3. FINE-TUNING CAUSAL
# ==========================================
generator = SABRCausalGenerator()
criterion = create_loss_function(
    loss_type='derivative', 
    value_weight=config['value_weight'], 
    derivative_weight=config['derivative_weight']
)

# Optimiseur avec LR r√©duit pour le corps du Transformer
optimizer = torch.optim.AdamW([
    {'params': model.encoder.parameters(), 'lr': config['lr'] * 0.05},
    {'params': model.custom_head.parameters(), 'lr': config['lr']}
])

print("üß† Lancement du Fine-tuning Causal...")
model.train()
for step in range(401):
    X_batch, y_batch = generator.generate(batch_size=config['batch_size'])
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)

    optimizer.zero_grad()
    out = model(X_batch)
    
    # Sobolev Loss (Vol + 6 Greeks)
    loss, _ = criterion(out[:, 0:1], y_batch[:, 0:1], 
                       {f'd{i}': out[:, i:i+1] for i in range(1, 7)},
                       {f'd{i}': y_batch[:, i:i+1] for i in range(1, 7)})
    loss.backward()
    optimizer.step()

    if step % 50 == 0:
        print(f"Step {step}/400 | Loss: {loss.item():.6f}")

# ==========================================
# 4. VISUALISATION (Le Causal Smile)
# ==========================================
def plot_results(model, generator):
    model.eval()
    # On g√©n√®re un smile complet (variation de K)
    X_test, y_true = generator.generate(batch_size=200)
    # Tri pour le graphique
    sort_idx = X_test[:, -1].argsort()
    X_test, y_true = X_test[sort_idx], y_true[sort_idx]
    
    with torch.no_grad():
        y_pred = model(X_test.to(device)).cpu()

    plt.figure(figsize=(10, 5))
    plt.plot(X_test[:, -1], y_true[:, 0], 'k--', label='Th√©orie SABR (Causal Graph)')
    plt.plot(X_test[:, -1], y_pred[:, 0], 'r-', label='Pr√©diction TabPFN-SABR', linewidth=2)
    plt.title("Step 9 : Validation du Fine-tuning Causal")
    plt.xlabel("Log-Moneyness (K/F)")
    plt.ylabel("Volatilit√©")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig("step9_causal_check.png")
    plt.show()

plot_results(model, generator)
torch.save(model.state_dict(), "tabpfn_sabr_causal_final.pth")
print("‚úÖ Mod√®le final sauvegard√©.")