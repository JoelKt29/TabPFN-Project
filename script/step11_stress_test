import json
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from tabpfn import TabPFNRegressor 
from step6_loss_with_derivatives import DerivativeLoss
from tqdm import tqdm

# --- CONFIGURATION DES CHEMINS ---
current_dir = Path(__file__).resolve().parent
data_dir = current_dir.parent / "data"
config_path = current_dir / "ray_results" / "best_config.json"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- ARCHITECTURE DU MOD√àLE ---
class StackingHead(nn.Module):
    def __init__(self, config, num_outputs):
        super().__init__()
        # Entr√©e : 8 features SABR + 1 pr√©diction TabPFN = 9
        h_dims = config.get('hidden_dims', [512, 256, 128])
        layers = []
        prev = 9 
        for h in h_dims:
            layers.extend([
                nn.Linear(prev, h), 
                nn.SiLU(), 
                nn.Dropout(config.get('dropout', 0.05))
            ])
            prev = h
        layers.append(nn.Linear(prev, num_outputs))
        self.net = nn.Sequential(*layers)

    def forward(self, x_raw, x_pfn):
        # Concat√©nation des features brutes et du "prior" de TabPFN
        return self.net(torch.cat([x_raw, x_pfn], dim=1))

def train_fast():
    print(f"üöÄ Initialisation sur {device}...")

    # 1. Chargement des fichiers
    with open(config_path, 'r') as f: 
        config = json.load(f)
    
    df = pd.read_csv(data_dir / 'sabr_with_derivatives_scaled.csv')

    # S√©lection automatique des colonnes (√©vite les KeyError)
    feature_cols = ['beta', 'rho', 'volvol', 'v_atm_n', 'alpha', 'F', 'K', 'log_moneyness']
    deriv_cols = [c for c in df.columns if c.startswith('dV_') and c.endswith('_scaled')]
    y_cols = ['volatility_scaled'] + deriv_cols
    
    X_r = df[feature_cols].values
    Y_r = df[y_cols].values
    num_outputs = len(y_cols)

    print(f"üìä Donn√©es charg√©es : {X_r.shape[0]} lignes.")
    print(f"üîé D√©riv√©es d√©tect√©es ({len(deriv_cols)}) : {deriv_cols}")

    # 2. Fit TabPFN (Contexte r√©duit √† 500 pour la vitesse)
    print("üéì Calibration du Prior TabPFN...")
    tabpfn = TabPFNRegressor(device='cuda' if torch.cuda.is_available() else 'cpu', n_estimators=4)
    idx = np.random.choice(len(X_r), 500, replace=False)
    tabpfn.fit(X_r[idx], Y_r[idx, 0])

    # 3. PHASE DE PRE-CACHING (Le secret de la vitesse)
    # On calcule les pr√©dictions TabPFN UNE SEULE FOIS pour tout le dataset
    print(f"üß† Pr√©-calcul des priors TabPFN (Pre-caching)...")
    pfn_priors = []
    chunk_size = 500 
    for i in tqdm(range(0, len(X_r), chunk_size), desc="Calcul des priors"):
        chunk = X_r[i:i+chunk_size]
        pfn_priors.append(tabpfn.predict(chunk).reshape(-1, 1))
    X_pfn = np.vstack(pfn_priors)

    # 4. Pr√©paration des DataLoaders
    X_train_raw, X_val_raw, X_train_pfn, X_val_pfn, Y_train, Y_val = train_test_split(
        X_r, X_pfn, Y_r, test_size=0.2, random_state=42
    )

    train_ds = TensorDataset(
        torch.FloatTensor(X_train_raw), 
        torch.FloatTensor(X_train_pfn), 
        torch.FloatTensor(Y_train)
    )
    train_loader = DataLoader(train_ds, batch_size=config.get('batch_size', 128), shuffle=True)

    # 5. Boucle d'entra√Ænement
    model = StackingHead(config, num_outputs).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.get('lr', 1e-4))
    criterion = DerivativeLoss(
        value_weight=config.get('value_weight', 1.0), 
        derivative_weight=config.get('derivative_weight', 0.5)
    )

    print(f"\nüî• D√©but de l'entra√Ænement ultra-rapide (50 √©poques)...")
    best_loss = float('inf')

    for epoch in range(50):
        model.train()
        total_loss = 0
        
        # Barre de progression par √©poque
        pbar = tqdm(train_loader, desc=f"√âpoque {epoch+1}/50", leave=False)
        
        for b_raw, b_pfn, b_y in pbar:
            b_raw, b_pfn, b_y = b_raw.to(device), b_pfn.to(device), b_y.to(device)
            
            # Ici, on utilise b_pfn d√©j√† calcul√©, c'est ce qui rend le code rapide !
            out = model(b_raw, b_pfn)
            
            # Pr√©paration des dictionnaires pour la DerivativeLoss
            pred_vol = out[:, 0:1]
            true_vol = b_y[:, 0:1]
            pred_der = {f'd{i}': out[:, i:i+1] for i in range(1, num_outputs)}
            true_der = {f'd{i}': b_y[:, i:i+1] for i in range(1, num_outputs)}
            
            loss, _ = criterion(pred_vol, true_vol, pred_der, true_der)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.5f}")

        avg_loss = total_loss / len(train_loader)
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "tabpfn_step9_causal_final.pth")

        if (epoch + 1) % 5 == 0:
            print(f"√âpoque {epoch+1:02d} | Loss Moyenne: {avg_loss:.6f}")

    print(f"\n‚úÖ Entra√Ænement termin√©. Meilleure Loss: {best_loss:.6f}")
    print("üíæ Mod√®le sauvegard√© sous : tabpfn_step9_causal_final.pth")

if __name__ == "__main__":
    train_fast()