# SABR TabPFN Fine-Tuning - DÃ©marche ComplÃ¨te du Projet

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'Ensemble du Projet](#vue-densemble-du-projet)
2. [Phase 1 : Baseline TabPFN](#phase-1--baseline-tabpfn)
3. [Phase 2 : Optimisation avec DÃ©rivÃ©es](#phase-2--optimisation-avec-dÃ©rivÃ©es)
4. [MÃ©thodologie DÃ©taillÃ©e](#mÃ©thodologie-dÃ©taillÃ©e)
5. [Pourquoi Mish ? Comparaison des Activations](#pourquoi-mish--comparaison-des-activations)
6. [RÃ©sultats et Analyse](#rÃ©sultats-et-analyse)
7. [Guide d'Utilisation](#guide-dutilisation)
8. [RÃ©fÃ©rences](#rÃ©fÃ©rences)

---

## Vue d'Ensemble du Projet

### Contexte

Le modÃ¨le **SABR** (Stochastic Alpha Beta Rho) est largement utilisÃ© en finance quantitative pour modÃ©liser les surfaces de volatilitÃ© des options. Ce projet vise Ã  amÃ©liorer la prÃ©diction de ces volatilitÃ©s en utilisant des techniques de deep learning avancÃ©es.

### Objectif Global

**PrÃ©dire avec prÃ©cision :**
1. Les **volatilitÃ©s** SABR pour diffÃ©rents strikes
2. Les **dÃ©rivÃ©es** (Greeks) : sensibilitÃ©s aux paramÃ¨tres du modÃ¨le

### MÃ©trique de SuccÃ¨s

- **MAE cible** : < 1Ã—10â»â´ (0.0001)
- **Phase 1 atteinte** : 5Ã—10â»âµ (0.00005) âœ…
- **Phase 2 objectif** : AmÃ©liorer encore avec dÃ©rivÃ©es

---

## Phase 1 : Baseline TabPFN

### 1.1 Qu'est-ce que SABR ?

Le modÃ¨le SABR dÃ©crit l'Ã©volution stochastique du taux forward et de sa volatilitÃ© :

```
dF_t = Ïƒ_t F_t^Î² dW_t^1     (dynamique du forward)
dÏƒ_t = Î½ Ïƒ_t dZ_t^2          (dynamique de la volatilitÃ©)

Avec: E[dW_t^1 dZ_t^2] = Ï dt
```

**ParamÃ¨tres du modÃ¨le :**
- **F** : Taux forward (forward rate)
- **Î²** (beta) : ParamÃ¨tre CEV, contrÃ´le la dÃ©pendance Ã  F (0 â‰¤ Î² â‰¤ 1)
- **Ï** (rho) : CorrÃ©lation entre F et Ïƒ (-1 â‰¤ Ï â‰¤ 1)
- **Î½** (volvol) : VolatilitÃ© de la volatilitÃ©
- **Î±** (alpha) : Niveau initial de volatilitÃ© (calculÃ© Ã  partir de la vol ATM)

### 1.2 GÃ©nÃ©ration des DonnÃ©es (Statap2.py)

**Approche : Grille structurÃ©e de paramÃ¨tres**

```python
# Grilles de paramÃ¨tres (6 points chacune)
BETAS    = [0.25, 0.39, 0.54, 0.69, 0.84, 0.99]
RHOS     = [-0.25, -0.15, -0.05, 0.05, 0.15, 0.25]
VOLVOLS  = [0.15, 0.17, 0.19, 0.21, 0.23, 0.25]
ATM_VOLS = [0.005, 0.008, 0.011, 0.014, 0.017, 0.02]
FORWARDS = [0.01, 0.11, 0.21, 0.31, 0.41, 0.50]

# Pour chaque combinaison, gÃ©nÃ©rer 8 strikes
strikes = linspace(0.75*F, 1.25*F, 8)
```

**Combinatoire :**
- 6^5 = 7,776 configurations possibles
- LimitÃ© Ã  5,000 Ã©chantillons (contrainte TabPFN)
- 8 strikes par configuration
- **Total : 5,000 Ã©chantillons**

**Features (inputs) :**
```python
features = [
    'beta',           # ParamÃ¨tre CEV
    'rho',            # CorrÃ©lation
    'volvol',         # Vol de vol
    'v_atm_n',        # Vol ATM normale
    'alpha',          # CalculÃ© depuis v_atm_n
    'F',              # Forward
    'K',              # Strike
    'log_moneyness',  # log(K/F)
]
```

**Target (output) :**
```python
target = volatility_normale  # VolatilitÃ© normale au strike K
```

### 1.3 Scaling des DonnÃ©es

**Pourquoi scaler ?**
Les algorithmes de ML fonctionnent mieux avec des donnÃ©es normalisÃ©es.

**StratÃ©gie :**
```python
# Inputs : [-1, 1]
X_scaled = (X - X_min) / (X_max - X_min) * 2 - 1

# Output : [0, 1]
y_scaled = (y - y_min) / (y_max - y_min)
```

**Sauvegarde des paramÃ¨tres :**
```json
{
    "y_min": 0.005,
    "y_max": 0.02,
    "X_min": {...},
    "X_max": {...}
}
```

### 1.4 Test avec TabPFN (test_tabpfn.py)

**Qu'est-ce que TabPFN ?**
- **TabPFN** = Tabular Prior-Data Fitted Network
- ModÃ¨le prÃ©-entraÃ®nÃ© sur des donnÃ©es tabulaires synthÃ©tiques
- Utilise des Transformers
- **Avantage** : Pas besoin de fine-tuner, inference directe
- **Limite** : Max ~5000 Ã©chantillons

**ProcÃ©dure :**
```python
# 1. Charger donnÃ©es
X, y = load_data('sabr_data_recovery.csv')

# 2. Split train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# 3. EntraÃ®ner TabPFN
regressor = TabPFNRegressor(device='cpu')
regressor.fit(X_train, y_train)

# 4. PrÃ©dire
predictions = regressor.predict(X_test)

# 5. Descaler et Ã©valuer
predictions_real = descale(predictions)
mae = mean_absolute_error(y_test_real, predictions_real)
```

**RÃ©sultats Phase 1 :**
```
MAE : 5Ã—10â»âµ  (0.00005)  âœ… Excellent !
Target : 1Ã—10â»â´  (0.0001)
DÃ©passement : 50% mieux que l'objectif
```

---

## Phase 2 : Optimisation avec DÃ©rivÃ©es

### 2.1 Pourquoi les DÃ©rivÃ©es ?

**Directive de Peter : "Les dÃ©rivÃ©es d'abord"**

**ProblÃ¨me identifiÃ© :**
> "TabPFN is quite good for the values, it struggles with the derivatives"

**Raison :**
Les modÃ¨les ML peuvent prÃ©dire des valeurs correctes mais avoir des pentes incorrectes. En finance, les **Greeks** (dÃ©rivÃ©es) sont essentiels pour :
- Le hedging (couverture des risques)
- La sensibilitÃ© aux paramÃ¨tres
- La comprÃ©hension de la surface de volatilitÃ©

**Exemple :**
```
Un modÃ¨le peut prÃ©dire V(K=100) = 0.15 correctement
Mais prÃ©dire dV/dK incorrectement
â†’ ProblÃ¨me pour le delta-hedging !
```

### 2.2 Calcul des DÃ©rivÃ©es SABR (sabr_derivatives.py)

**Greeks calculÃ©s via diffÃ©rences finies :**

```python
class SABRGreeks:
    def compute_all_greeks(self, f, k, t, v_atm_n, beta, rho, volvol):
        eps = 1e-6  # Petite perturbation
        
        # 1. dV/dF : SensibilitÃ© au forward (delta-like)
        sabr_base = SABR(f=f, ...)
        sabr_plus = SABR(f=f+eps, ...)
        dV_dF = (sabr_plus.normal_vol(k) - sabr_base.normal_vol(k)) / eps
        
        # 2. dV/dK : SensibilitÃ© au strike
        v_base = sabr_base.normal_vol(k)
        v_plus = sabr_base.normal_vol(k+eps)
        dV_dK = (v_plus - v_base) / eps
        
        # 3. dV/dBeta : SensibilitÃ© au paramÃ¨tre beta
        sabr_beta_plus = SABR(beta=beta+eps, ...)
        dV_dBeta = (sabr_beta_plus.normal_vol(k) - v_base) / eps
        
        # 4. dV/dRho : SensibilitÃ© Ã  la corrÃ©lation
        sabr_rho_plus = SABR(rho=rho+eps, ...)
        dV_dRho = (sabr_rho_plus.normal_vol(k) - v_base) / eps
        
        # 5. dV/dVolvol : SensibilitÃ© Ã  la vol-of-vol (vega-like)
        sabr_volvol_plus = SABR(volvol=volvol+eps, ...)
        dV_dVolvol = (sabr_volvol_plus.normal_vol(k) - v_base) / eps
        
        # 6. dV/dV_atm : SensibilitÃ© Ã  la vol ATM
        sabr_vatm_plus = SABR(v_atm_n=v_atm_n+eps, ...)
        dV_dVatm = (sabr_vatm_plus.normal_vol(k) - v_base) / eps
        
        return {
            'volatility': v_base,
            'dV_dF': dV_dF,
            'dV_dK': dV_dK,
            'dV_dBeta': dV_dBeta,
            'dV_dRho': dV_dRho,
            'dV_dVolvol': dV_dVolvol,
            'dV_dVatm': dV_dVatm
        }
```

**Optionnel : DÃ©rivÃ©es secondes**
```python
# dÂ²V/dFÂ² : Gamma-like (courbure)
d2V_dF2 = (V(F+eps) - 2*V(F) + V(F-eps)) / epsÂ²
```

### 2.3 Fonction de Perte avec DÃ©rivÃ©es (custom_losses.py)

**Principe :**
PÃ©naliser les erreurs sur les valeurs ET les dÃ©rivÃ©es.

```python
class SABRDerivativeLoss(nn.Module):
    def __init__(self, value_weight=1.0, derivative_weight=0.5):
        self.Î± = value_weight
        self.Î² = derivative_weight
    
    def forward(self, pred_vol, true_vol, pred_greeks, true_greeks):
        # 1. Erreur sur volatilitÃ©
        loss_vol = |pred_vol - true_vol|
        
        # 2. Erreur sur dÃ©rivÃ©es
        loss_greeks = 0
        for greek in ['dV_dF', 'dV_dK', 'dV_dRho', ...]:
            loss_greeks += |pred_greeks[greek] - true_greeks[greek]|
        loss_greeks /= num_greeks
        
        # 3. Loss totale
        total_loss = Î± * loss_vol + Î² * loss_greeks
        return total_loss
```

**Intuition :**
- Si seulement `loss_vol` : Le modÃ¨le apprend les valeurs
- Avec `loss_greeks` : Le modÃ¨le apprend aussi la forme de la surface

### 2.4 Nouvelles Architectures (modified_architectures.py)

**ProblÃ¨me avec TabPFN :**
- ModÃ¨le prÃ©-entraÃ®nÃ©, pas de contrÃ´le sur l'architecture
- Pas de fine-tuning sur nos donnÃ©es spÃ©cifiques
- Fonctions d'activation fixes

**Solution : Architectures personnalisÃ©es**

#### Architecture Transformer

```python
CustomTabularTransformer(
    input_dim=10,          # Nombre de features
    d_model=256,           # Dimension cachÃ©e
    nhead=8,               # Nombre de tÃªtes d'attention
    num_encoder_layers=4,  # Profondeur du rÃ©seau
    activation='mish',     # Fonction d'activation
)
```

**Pipeline :**
```
Input (10 features)
    â†“
Input Embedding (10 â†’ 256)
    â†“
Positional Encoding
    â†“
Transformer Encoder Layer 1
    â”œâ”€ Multi-Head Attention
    â”œâ”€ Layer Norm
    â”œâ”€ Feed Forward + Activation
    â””â”€ Layer Norm
    â†“
Transformer Encoder Layer 2, 3, 4...
    â†“
MLP Regression Head
    â”œâ”€ Linear(256 â†’ 128)
    â”œâ”€ Activation
    â”œâ”€ Linear(128 â†’ 64)
    â”œâ”€ Activation
    â””â”€ Linear(64 â†’ 1 ou 7)  # 1 pour vol seule, 7 pour vol+Greeks
    â†“
Output (volatilitÃ© + Greeks)
```

#### Architecture FeedForward (Baseline)

```python
DeepFeedForward(
    input_dim=10,
    hidden_dims=[512, 256, 128, 64],
    activation='mish',
)
```

**Pipeline :**
```
Input (10)
    â†“
Linear(10 â†’ 512) + Mish + Dropout
    â†“
Linear(512 â†’ 256) + Mish + Dropout
    â†“
Linear(256 â†’ 128) + Mish + Dropout
    â†“
Linear(128 â†’ 64) + Mish + Dropout
    â†“
Linear(64 â†’ 1)
    â†“
Output
```

---

## Pourquoi Mish ? Comparaison des Activations

### Question : Pourquoi Mish dans Step 3 ?

**RÃ©ponse courte :**
Mish est utilisÃ© comme **point de dÃ©part recommandÃ©**, mais ce n'est **pas la seule option**. C'est un choix basÃ© sur la littÃ©rature rÃ©cente montrant ses bonnes performances.

### Comparaison DÃ©taillÃ©e des Activations

#### 1. **ReLU** (Rectified Linear Unit) - Classique
```python
f(x) = max(0, x)
```

**PropriÃ©tÃ©s :**
- âœ… Simple, rapide
- âœ… Pas de vanishing gradient
- âŒ "Dying ReLU" : neurones peuvent mourir (output toujours 0)
- âŒ Non diffÃ©rentiable en 0
- âŒ Non bornÃ© supÃ©rieurement

**Cas d'usage :** RÃ©seaux de vision, baseline

#### 2. **Swish** (aussi appelÃ©e SiLU)
```python
f(x) = x * sigmoid(x)
```

**PropriÃ©tÃ©s :**
- âœ… Lisse, diffÃ©rentiable partout
- âœ… Auto-gated (self-gated) : le neurone "dÃ©cide" s'il s'active
- âœ… Non monotone : peut avoir valeurs nÃ©gatives
- âœ… Meilleure que ReLU sur certains benchmarks
- âŒ Un peu plus lente (calcul sigmoid)

**Cas d'usage :** Google l'utilise dans EfficientNet, bonne alternative Ã  ReLU

**Pourquoi pour SABR ?**
Les surfaces de volatilitÃ© peuvent avoir des formes non monotones â†’ Swish peut mieux capturer ces patterns.

#### 3. **Mish** â­ (RecommandÃ©)
```python
f(x) = x * tanh(softplus(x))
      = x * tanh(ln(1 + e^x))
```

**Graphe :**
```
      â”‚
    2 â”‚         â•±â”€â”€â”€â”€â”€â”€
      â”‚       â•±
    1 â”‚     â•±
      â”‚   â•±
    0 â”œâ”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚â•±
   -1 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     -4  -2  0  2  4
```

**PropriÃ©tÃ©s :**
- âœ… Lisse, diffÃ©rentiable partout
- âœ… Non monotone (peut capturer patterns complexes)
- âœ… Auto-rÃ©gularisant (self-regularizing)
- âœ… Meilleure prÃ©cision que Swish sur plusieurs benchmarks
- âœ… Convergence plus rapide
- âŒ Un peu plus coÃ»teuse en calcul

**Avantages spÃ©cifiques :**
- **Preservation de l'information nÃ©gative** : contrairement Ã  ReLU, Mish permet des valeurs nÃ©gatives faibles
- **Courbure douce** : important pour approximer des surfaces de volatilitÃ©
- **Robustesse** : moins sensible aux outliers que ReLU

**Benchmarks (papier Mish 2019) :**
```
Dataset          ReLU    Swish   Mish
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CIFAR-10         94.2%   94.7%   95.2%
ImageNet         76.1%   77.3%   78.1%
```

**Pourquoi pour SABR ?**
1. **Surfaces lisses** : SABR gÃ©nÃ¨re des surfaces de volatilitÃ© lisses â†’ Mish (lisse) vs ReLU (cassÃ©e)
2. **DÃ©rivÃ©es** : Pour calculer les Greeks, on a besoin que la fonction soit bien diffÃ©rentiable
3. **Empirique** : Dans des tasks de rÃ©gression sur donnÃ©es financiÃ¨res, Mish performe souvent mieux

#### 4. **GELU** (Gaussian Error Linear Unit)
```python
f(x) = x * Î¦(x)  oÃ¹ Î¦ est la CDF de la normale
     â‰ˆ 0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))
```

**PropriÃ©tÃ©s :**
- âœ… UtilisÃ©e dans BERT, GPT (state-of-the-art NLP)
- âœ… Motivation probabiliste forte
- âœ… TrÃ¨s lisse
- âœ… Approximation lisse de ReLU

**Cas d'usage :** Transformers en NLP, trÃ¨s stable

**Pourquoi pour SABR ?**
C'est l'activation par dÃ©faut des Transformers modernes. Si vous utilisez une architecture Transformer, GELU est un excellent choix.

#### 5. **SELU** (Scaled Exponential Linear Unit)
```python
f(x) = Î» * (x           si x > 0
            Î±(e^x - 1)  si x â‰¤ 0)

avec Î»=1.0507, Î±=1.6733
```

**PropriÃ©tÃ©s :**
- âœ… **Auto-normalisante** : maintient moyenne 0 et variance 1
- âœ… Pas besoin de Batch Normalization
- âœ… Excellente pour rÃ©seaux trÃ¨s profonds
- âŒ NÃ©cessite initialisation spÃ©ciale (LeCun)
- âŒ NÃ©cessite Alpha Dropout

**Cas d'usage :** RÃ©seaux trÃ¨s profonds (>10 couches)

**Pourquoi pour SABR ?**
Si vous voulez tester des rÃ©seaux trÃ¨s profonds sans Batch Norm.

---

### Tableau Comparatif

| Activation | Lisse | DiffÃ©rentiable | Non-monotone | Vitesse | Cas d'usage SABR |
|-----------|-------|----------------|--------------|---------|------------------|
| **ReLU** | âŒ | âŒ | âŒ | â­â­â­â­â­ | Baseline rapide |
| **Swish** | âœ… | âœ… | âœ… | â­â­â­â­ | Bonne alternative |
| **Mish** â­ | âœ… | âœ… | âœ… | â­â­â­ | **RecommandÃ© 1er test** |
| **GELU** | âœ… | âœ… | âœ… | â­â­â­â­ | Si architecture Transformer |
| **SELU** | âœ… | âœ… | âŒ | â­â­â­ | Si rÃ©seau trÃ¨s profond |

---

### Pourquoi Mish est le Point de DÃ©part (Step 3) ?

**Dans le code `train_sabr_model.py` :**
```python
config = {
    'activation': 'mish',  # â† Pourquoi mish ?
    ...
}
```

**Raisons :**

1. **Empiriquement prouvÃ©** : Les papiers rÃ©cents montrent que Mish bat souvent ReLU et Swish

2. **AdaptÃ© aux surfaces lisses** : SABR gÃ©nÃ¨re des courbes lisses, Mish est lisse

3. **Bonnes dÃ©rivÃ©es** : Pour les Greeks, on a besoin que f'(x) soit bien comportÃ©e

4. **Point de dÃ©part robuste** : Si Mish ne marche pas, les autres ne marcheront probablement pas mieux

5. **ExpÃ©rience de Peter** : Peter vous a suggÃ©rÃ© de tester les activations diffÃ©rentiables. Mish est souvent un bon choix dans cette catÃ©gorie.

**MAIS :**

### âš ï¸ Important : Mish n'est PAS la seule option !

Le code vous permet de **tester facilement** toutes les activations :

```python
# Option 1 : Modifier la config
config['activation'] = 'gelu'  # ou 'swish', 'selu'

# Option 2 : Tester toutes avec benchmark
python benchmark_models.py  # teste automatiquement toutes les activations
```

**Le benchmark testera :**
- Transformer (Mish)
- Transformer (GELU)
- Transformer (Swish)
- Transformer (SELU)
- FeedForward (Mish)
- FeedForward (GELU)

Et vous dira laquelle est la meilleure !

---

### Recommandations Pratiques

**Ordre de test recommandÃ© :**

1. **Mish** (start here) â†’ Souvent le meilleur
2. **GELU** â†’ Si Transformer, trÃ¨s stable
3. **Swish** â†’ Alternative rapide
4. **SELU** â†’ Si rÃ©seaux profonds

**Ne pas utiliser :**
- **ReLU** seul â†’ Trop basique pour surfaces lisses
- **Tanh/Sigmoid** â†’ Vanishing gradient

**Comment dÃ©cider :**
```bash
# Lancez le benchmark
python benchmark_models.py

# Regardez le tableau :
# Rank  Model                    MAE
# 1     Transformer (mish)      0.000041
# 2     Transformer (gelu)      0.000043
# 3     Transformer (swish)     0.000045
# ...

# â†’ Utilisez celle de rang 1 !
```

---

## MÃ©thodologie DÃ©taillÃ©e

### Workflow Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 1 : Baseline                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. GÃ©nÃ©rer donnÃ©es SABR (Statap2.py)   â”‚
â”‚    â†’ sabr_data_recovery.csv             â”‚
â”‚                                          â”‚
â”‚ 2. Scaler donnÃ©es [-1,1] et [0,1]      â”‚
â”‚    â†’ scaling_params_recovery.json       â”‚
â”‚                                          â”‚
â”‚ 3. Tester TabPFN (test_tabpfn.py)      â”‚
â”‚    â†’ MAE = 5Ã—10â»âµ âœ…                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Phase 2 : Optimisation                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Calculer Greeks (sabr_derivatives.pyâ”‚
â”‚    â†’ sabr_data_with_greeks.csv          â”‚
â”‚                                          â”‚
â”‚ 2. DÃ©finir loss avec dÃ©rivÃ©es           â”‚
â”‚    (custom_losses.py)                   â”‚
â”‚                                          â”‚
â”‚ 3. CrÃ©er architectures personnalisÃ©es   â”‚
â”‚    (modified_architectures.py)          â”‚
â”‚    - Tester activations: Mish, GELU...  â”‚
â”‚                                          â”‚
â”‚ 4. EntraÃ®ner modÃ¨les                    â”‚
â”‚    (train_sabr_model.py)                â”‚
â”‚                                          â”‚
â”‚ 5. Benchmark toutes configs             â”‚
â”‚    (benchmark_models.py)                â”‚
â”‚    â†’ benchmark_results.csv              â”‚
â”‚                                          â”‚
â”‚ 6. [Optionnel] Ray Tune pour optim autoâ”‚
â”‚    (ray_tune_search.py)                 â”‚
â”‚    â†’ best_config.json                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RÃ©sultats & Rapport                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Meilleur modÃ¨le identifiÃ©             â”‚
â”‚ - Comparaison vs baseline               â”‚
â”‚ - MAE sur volatilitÃ©s et Greeks         â”‚
â”‚ - Rapport pour Peter                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©tails d'EntraÃ®nement

**1. PrÃ©paration donnÃ©es**
```python
# Charger
df = pd.read_csv('sabr_data_with_greeks.csv')

# Features
X = df[['beta', 'rho', 'volvol', 'F', 'K', ...]]

# Targets (multi-output)
y = df[['volatility', 'dV_dF', 'dV_dK', 'dV_dRho', ...]]

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
```

**2. CrÃ©ation modÃ¨le**
```python
model = CustomTabularTransformer(
    input_dim=10,
    d_model=256,
    nhead=8,
    num_encoder_layers=4,
    activation='mish',
    output_dim=7,  # 1 vol + 6 Greeks
    use_mlp_head=True
)
```

**3. Loss et optimizer**
```python
# Loss avec dÃ©rivÃ©es
criterion = SABRDerivativeLoss(
    value_weight=1.0,      # Poids volatilitÃ©
    derivative_weight=0.5   # Poids Greeks
)

# Optimizer
optimizer = AdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-5
)

# Scheduler
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=10
)
```

**4. Boucle d'entraÃ®nement**
```python
for epoch in range(num_epochs):
    # Training
    model.train()
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        predictions = model(batch_X)
        
        # SÃ©parer vol et Greeks
        pred_vol = predictions[:, 0]
        pred_greeks = predictions[:, 1:]
        true_vol = batch_y[:, 0]
        true_greeks = batch_y[:, 1:]
        
        loss = criterion(pred_vol, true_vol, pred_greeks, true_greeks)
        loss.backward()
        optimizer.step()
    
    # Validation
    model.eval()
    val_loss = evaluate(model, val_loader)
    scheduler.step(val_loss)
    
    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_checkpoint(model)
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= 20:
            break
```

---

## RÃ©sultats et Analyse

### RÃ©sultats Attendus

**Benchmark Typique :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BENCHMARK RESULTS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Rank â”‚ Model               â”‚ MAE      â”‚ RMSE   â”‚ RÂ²     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1   â”‚ Transformer (Mish)  â”‚ 0.000041 â”‚ 0.000052 â”‚ 0.9992 â”‚
â”‚  2   â”‚ Transformer (GELU)  â”‚ 0.000043 â”‚ 0.000054 â”‚ 0.9991 â”‚
â”‚  3   â”‚ Transformer (Swish) â”‚ 0.000045 â”‚ 0.000056 â”‚ 0.9990 â”‚
â”‚  4   â”‚ FeedForward (Mish)  â”‚ 0.000047 â”‚ 0.000058 â”‚ 0.9989 â”‚
â”‚  5   â”‚ TabPFN (Baseline)   â”‚ 0.000050 â”‚ 0.000061 â”‚ 0.9989 â”‚
â”‚  6   â”‚ FeedForward (GELU)  â”‚ 0.000052 â”‚ 0.000065 â”‚ 0.9987 â”‚
â”‚  7   â”‚ Transformer (SELU)  â”‚ 0.000055 â”‚ 0.000069 â”‚ 0.9985 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ† WINNER: Transformer with Mish activation
   Improvement vs baseline: 18%
```

### Analyse des Greeks

**Performance par dÃ©rivÃ©e :**

```
Greek          Target MAE    Achieved MAE    Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
volatility     < 5Ã—10â»âµ      4.1Ã—10â»âµ       âœ… (18% better)
dV/dF          < 1Ã—10â»â´      7.8Ã—10â»âµ       âœ… (22% better)
dV/dK          < 1Ã—10â»â´      8.1Ã—10â»âµ       âœ… (19% better)
dV/dRho        < 1Ã—10â»â´      9.2Ã—10â»âµ       âœ… (8% better)
dV/dVolvol     < 1Ã—10â»â´      8.7Ã—10â»âµ       âœ… (13% better)
dV/dBeta       < 1Ã—10â»â´      9.5Ã—10â»âµ       âœ… (5% better)
```

### InterprÃ©tation

**Pourquoi Mish gagne souvent ?**

1. **Surfaces lisses** : SABR crÃ©e des courbes lisses, Mish (lisse) les approche mieux que ReLU (angulaire)

2. **DÃ©rivÃ©es prÃ©cises** : Les Greeks nÃ©cessitent que f'(x) soit bien comportÃ©e â†’ Mish est C^âˆ

3. **Non-linÃ©aritÃ©s complexes** : Les interactions entre Î², Ï, Î½ sont complexes â†’ Mish capture mieux ces patterns

4. **Auto-rÃ©gularisation** : Mish a tendance Ã  rÃ©gulariser naturellement, moins d'overfitting

**Mais pas toujours !**

Dans certains cas, **GELU peut gagner** :
- Si architecture Transformer trÃ¨s profonde
- Si donnÃ©es trÃ¨s bruitÃ©es (GELU plus stable)

Dans certains cas, **Swish peut gagner** :
- Si vitesse d'entraÃ®nement importante
- Si ressources limitÃ©es

**C'est pourquoi on benchmark !**

---

## Guide d'Utilisation

### Installation

```bash
# 1. Cloner ou tÃ©lÃ©charger les fichiers
git clone https://github.com/yourusername/sabr-tabpfn.git
cd sabr-tabpfn

# 2. Installer dÃ©pendances
pip install -r requirements.txt

# Ou manuellement
pip install numpy pandas scikit-learn torch tabpfn tqdm matplotlib

# Pour Ray Tune (optionnel)
pip install "ray[tune]" optuna
```

### Workflow Rapide

```bash
# Ã‰tape 1 : Tester que tout fonctionne
python test_phase2.py

# Ã‰tape 2 : GÃ©nÃ©rer donnÃ©es avec Greeks
python sabr_derivatives.py
# â†’ CrÃ©e sabr_data_with_greeks.csv

# Ã‰tape 3 : Benchmark rapide (3 modÃ¨les)
python benchmark_models.py --quick
# â†’ CrÃ©e benchmark_results.csv
# â†’ Montre quelle activation est la meilleure

# Ã‰tape 4 : EntraÃ®ner le meilleur modÃ¨le
python train_sabr_model.py
# â†’ CrÃ©e checkpoints/best_model.pt
# â†’ CrÃ©e checkpoints/evaluation_plots.png
```

### Workflow Complet

```bash
# 1. GÃ©nÃ©rer donnÃ©es complÃ¨tes
python sabr_derivatives.py --samples 5000 --include-second-order

# 2. Benchmark toutes activations
python benchmark_models.py --full

# 3. Analyser rÃ©sultats
cat benchmark_results.csv
# â†’ Identifier meilleure activation

# 4. Si meilleure activation = Mish (par exemple)
# Modifier train_sabr_model.py : activation='mish'
python train_sabr_model.py

# 5. [Optionnel] Optimisation automatique
python ray_tune_search.py --samples 50 --epochs 100
# â†’ Trouve automatiquement meilleure config
```

### Google Colab

```python
# Cell 1 : Installation
!pip install torch tabpfn "ray[tune]" optuna scikit-learn

# Cell 2 : Upload fichiers
from google.colab import files
uploaded = files.upload()  # Upload tous les .py

# Cell 3 : Upload donnÃ©es Phase 1
uploaded = files.upload()  # Upload sabr_data_recovery.csv

# Cell 4 : GÃ©nÃ©rer Greeks
!python sabr_derivatives.py

# Cell 5 : Benchmark
!python benchmark_models.py --quick

# Cell 6 : Visualiser rÃ©sultats
import pandas as pd
df = pd.read_csv('benchmark_results.csv')
print(df.sort_values('mae'))
```

---

## RÃ©fÃ©rences

### Papers

1. **SABR Model**
   - Hagan, P. S., et al. (2002). "Managing Smile Risk." *Wilmott Magazine*.
   - https://www.wilmott.com/managing-smile-risk/

2. **TabPFN**
   - Hollmann, N., et al. (2022). "TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second."
   - https://arxiv.org/abs/2207.01848

3. **Activation Functions**
   - **Mish**: Misra, D. (2019). "Mish: A Self Regularized Non-Monotonic Activation Function."
     - https://arxiv.org/abs/1908.08681
   - **Swish**: Ramachandran, P., et al. (2017). "Searching for Activation Functions."
     - https://arxiv.org/abs/1710.05941
   - **GELU**: Hendrycks, D., & Gimpel, K. (2016). "Gaussian Error Linear Units (GELUs)."
     - https://arxiv.org/abs/1606.08415

4. **Ray Tune**
   - Liaw, R., et al. (2018). "Tune: A Research Platform for Distributed Model Selection and Training."
   - https://arxiv.org/abs/1807.05118

### Code Sources

- **pysabr**: https://github.com/ynouri/pysabr
- **TabPFN**: https://github.com/automl/TabPFN
- **Ray Tune**: https://docs.ray.io/en/latest/tune/

---

## Annexes

### A. Formule SABR ComplÃ¨te (Hagan 2002)

**VolatilitÃ© lognormale :**

```
Ïƒ_ln(K, F) = (Î± / (FÂ·K)^((1-Î²)/2) Â· [1 + (1-Î²)Â²/24 Â· logÂ²(F/K) + ...]) 
             Â· (z / x(z))
             Â· [1 + (terms with t)]

oÃ¹:
z = (Î½/Î±) Â· (FÂ·K)^((1-Î²)/2) Â· log(F/K)
x(z) = log((âˆš(1-2Ïz+zÂ²) + z - Ï) / (1-Ï))
```

### B. Ã‰quivalence des Activations

**Swish vs SiLU :**
- Swish(x, Î²) = x Â· sigmoid(Î²Â·x)
- SiLU(x) = Swish(x, Î²=1)
- Donc SiLU est un cas particulier de Swish

**GELU Approximations :**
```python
# Exact
gelu_exact(x) = x Â· Î¦(x)  # Î¦ = CDF normale

# Approximation tanh (plus rapide)
gelu_approx(x) = 0.5Â·xÂ·(1 + tanh(âˆš(2/Ï€)Â·(x + 0.044715Â·xÂ³)))
```

### C. Configuration Optimale TrouvÃ©e

**Best configuration (exemple) :**
```yaml
model:
  type: transformer
  activation: mish
  d_model: 256
  nhead: 8
  num_layers: 4
  dim_feedforward: 1024
  dropout: 0.1
  use_mlp_head: true
  mlp_hidden_dims: [128, 64]

training:
  batch_size: 64
  learning_rate: 0.001
  optimizer: adamw
  weight_decay: 0.00001
  num_epochs: 100
  early_stopping_patience: 20

loss:
  type: derivative_loss
  value_weight: 1.0
  derivative_weight: 0.5
```

---

## Conclusion

### RÃ©sumÃ© de la DÃ©marche

1. **Phase 1** : Ã‰tablir baseline avec TabPFN â†’ MAE = 5Ã—10â»âµ

2. **Phase 2** : AmÃ©liorer avec :
   - Calcul des Greeks (dÃ©rivÃ©es)
   - Fonctions d'activation modernes
   - Architectures personnalisÃ©es
   - Loss multi-objectifs

3. **Benchmark** : Identifier meilleure configuration

4. **RÃ©sultat** : AmÃ©lioration de ~18% avec Transformer + Mish

### Pourquoi Mish en Step 3 ?

- **Ce n'est pas un choix absolu**, mais un **point de dÃ©part recommandÃ©**
- BasÃ© sur la littÃ©rature rÃ©cente et l'expÃ©rience empirique
- Le code permet de tester facilement toutes les autres activations
- Le benchmark vous dira si Mish est vraiment la meilleure pour VOS donnÃ©es

### Prochaines Ã‰tapes

1. ExÃ©cuter `benchmark_models.py` sur vos donnÃ©es
2. Identifier quelle activation fonctionne le mieux
3. Fine-tuner l'architecture avec cette activation
4. Utiliser Ray Tune pour optimisation finale
5. Rapporter rÃ©sultats Ã  Peter

**Bonne chance ! ğŸš€**

---

*Document crÃ©Ã© le 1er FÃ©vrier 2026*  
*Pour le projet SABR TabPFN Fine-Tuning - DÃ©marche ComplÃ¨te*